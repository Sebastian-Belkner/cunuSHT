

// # Wrapper function to call the CUDA kernel
extern "C" void Legendre(int l_max, int m_max, double *host_x, double *host_result, int size) {
    float *device_x, *device_result;

    // Allocate device memory
    cudaMalloc((void **)&device_x, size * sizeof(float));
    cudaMalloc((void **)&device_result, size * (l_max + 1) * sizeof(float));

    // Copy data from host to device
    cudaMemcpy(device_x, host_x, size * sizeof(float), cudaMemcpyHostToDevice);

    // Launch kernel
    int threadsPerBlock = 256;
    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;
    kernel_legendre<<<blocksPerGrid, threadsPerBlock>>>(size* (l_max + 1), device_x, l_max, device_result);
    cudaDeviceSynchronize();

    // Copy result from device to host
    cudaMemcpy(host_result, device_result, size * (l_max + 1) * sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(device_x);
    cudaFree(device_result);

    // Print results (for demonstration)
    // for (int i = 0; i < n; ++i) {
    for (int l = 0; l <= l_max; ++l) {
        printf("P%d(x)= ", l);
        for (int i = 0; i < size; ++i) {
            printf("%f ", host_result[i*(l+1)]);
        }
        printf("\n");
    }
        // printf("\n");
    // }
}


// __device__ double d_legendre(int l, int m, double x) {
//     if (l == 0 && m == 0) {
//         return 1.0;
//     } else if (l == 1 && m == 0) {
//         return x;
//     } else if (l == 1 && m == 1) {
//         return -sqrt(1 - x * x);
//     } else if (l == m) {
//         return -(2 * m - 1) * sqrt(1 - x * x) * d_legendre(l - 1, m - 1, x);
//     } else if (l == m + 1) {
//         return (2 * m + 1) * x * d_legendre(m, m, x);
//     } else {
//         return ((2 * l - 1) * x * d_legendre(l - 1, m, x) - (l + m - 1) * d_legendre(l - 2, m, x)) / (l - m);
//     }
// }

// __global__ void Legendre(int l_max, int m_max, double *x, double *result) {
//     int idx = blockIdx.x * blockDim.x + threadIdx.x;
//     // if (idx < l_max + 1) {
//         double xi = x[idx];
//         // for (int l = idx; l <= l_max; ++l) {
//         result[idx] = d_legendre(l_max, m_max, xi);
//         // }
//     }

// // # Wrapper function to call the CUDA kernel
// extern "C" void LegendreCuda(int l_max, int m_max, double *x, double *result, int size) {
//     double *d_x, *d_result;
//     cudaMalloc((void**)&d_x, sizeof(x) / sizeof(x[0]) * sizeof(double));
//     cudaMalloc((void**)&d_result, sizeof(x) / sizeof(x[0]) * sizeof(double));
    
//     cudaEvent_t start, stop;
//     cudaEventCreate(&start);
//     cudaEventCreate(&stop);

//     cudaMemcpy(d_x, x, sizeof(x) / sizeof(x[0]) * sizeof(double), cudaMemcpyHostToDevice);

//     int threadsPerBlock = 512;
//     int blocksPerGrid = (l_max + 1 + threadsPerBlock - 1) / threadsPerBlock;

//     cudaEventRecord(start);
//     Legendre<<<blocksPerGrid, threadsPerBlock>>>(l_max, m_max, d_x, d_result);
//     cudaEventRecord(stop);

//     cudaError_t errSync  = cudaGetLastError();
//     cudaError_t errAsync = cudaDeviceSynchronize();
//     if (errSync != cudaSuccess) 
//     printf("Sync kernel error: %s\n", cudaGetErrorString(errSync));
//     if (errAsync != cudaSuccess)
//     printf("Async kernel error: %s\n", cudaGetErrorString(errAsync));

//     cudaError_t cuda_error = cudaGetLastError();
//     if (cuda_error != cudaSuccess) {
//         printf("CUDA error: %s\n", cudaGetErrorString(cuda_error));
//     }

//     cudaMemcpy(result, d_result, sizeof(x) / sizeof(x[0]) * sizeof(double), cudaMemcpyDeviceToHost);

//     cudaEventSynchronize(stop);
//     float milliseconds = 0;
//     cudaEventElapsedTime(&milliseconds, start, stop);

//     printf("Effective Bandwidth (GB/s): %fn", l_max*4*3/milliseconds/1e6);
//     cudaFree(d_x);
//     cudaFree(d_result);
// }