{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cunusht'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcunusht\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cunusht'"
     ]
    }
   ],
   "source": [
    "import cunusht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHTns 3.6.6] built May 27 2024, 07:48:01, id: v3.6.6-11-gf0a006a,avx512,ishioka,openmp\n"
     ]
    }
   ],
   "source": [
    "import shtns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHTns 3.6.6] built May 27 2024, 07:48:01, id: v3.6.6-11-gf0a006a,avx512,ishioka,openmp,cuda\n",
      "initializing shtns for CC in GPU_SHTns_transformer\n",
      "geominfo for CC in GPU_SHTns_transformer:  ('cc', {'lmax': 2559, 'mmax': 2559, 'ntheta': 2592, 'nphi': 5120})\n",
      "[SHTns 3.6.6] built May 27 2024, 07:48:01, id: v3.6.6-11-gf0a006a,avx512,ishioka,openmp,cuda\n",
      "        Lmax=2559, Mmax*Mres=2559, Mres=1, Nlm=3278080  [10 threads, orthonormalized]\n",
      "        > Condon-Shortley phase = 1, normalization = 0\n",
      "!WARNING! Nlat must be larger than 2*Lmax for analysis to work (sampling theorem)!        => using FFTW : Mmax=2559, Nphi=5120, Nlat=2592, Nbatch=1  (theta-contiguous layout: phi_inc=2592, theta_inc=1)\n",
      "          fftw cost ifftc=1.10653e+08,  fftc=1.10653e+08\n",
      "          Memory used for Ylm and Zlm matrices = 97237.969 Mb x2\n",
      "        => using Regular nodes including poles, with Clenshaw-Curtis quadrature\n",
      "     !! Warning : Regular-Clenshaw-Curtis anti-aliasing condition Nlat > 2*Lmax is not met.\n",
      "          Sum of weights = 2 + 1.33227e-15 (should be 2)\n",
      "          Applying quadrature rule to 3/2.x^2 = 1 + 1.11022e-16 (should be 1)\n",
      "          Applying quadrature rule to 3/4.sin2(theta) = 1 + 2.77556e-16 (should be 1)\n",
      "        + polar optimization threshold = 1.0e-10\n",
      "          tm[im]= 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 6 6 6 6 7 7 7 7 7 8 8 8 8 9 9 9 9 10 10 10 10 11 11 11 11 12 12 12 12 13 13 13 14 14 14 14 15 15 15 15 16 16 16 16 17 17 17 18 18 18 18 19 19 19 19 20 20 20 21 21 21 21 22 22 22 22 23 23 23 24 24 24 24 25 25 25 26 26 26 26 27 27 27 28 28 28 28 29 29 29 30 30 30 30 31 31 31 31 32 32 32 33 33 33 34 34 34 34 35 35 35 36 36 36 36 37 37 37 38 38 38 38 39 39 39 40 40 40 40 41 41 41 42 42 42 42 43 43 43 44 44 44 45 45 45 45 46 46 46 47 47 47 47 48 48 48 49 49 49 50 50 50 50 51 51 51 52 52 52 53 53 53 53 54 54 54 55 55 55 55 56 56 56 57 57 57 58 58 58 58 59 59 59 60 60 60 61 61 61 61 62 62 62 63 63 63 64 64 64 64 65 65 65 66 66 66 67 67 67 67 68 68 68 69 69 69 70 70 70 70 71 71 71 72 72 72 73 73 73 73 74 74 74 75 75 75 76 76 76 77 77 77 77 78 78 78 79 79 79 80 80 80 80 81 81 81 82 82 82 83 83 83 83 84 84 84 85 85 85 86 86 86 87 87 87 87 88 88 88 89 89 89 90 90 90 90 91 91 91 92 92 92 93 93 93 94 94 94 94 95 95 95 96 96 96 97 97 97 98 98 98 98 99 99 99 100 100 100 101 101 101 102 102 102 102 103 103 103 104 104 104 105 105 105 106 106 106 106 107 107 107 108 108 108 109 109 109 110 110 110 110 111 111 111 112 112 112 113 113 113 114 114 114 115 115 115 115 116 116 116 117 117 117 118 118 118 119 119 119 119 120 120 120 121 121 121 122 122 122 123 123 123 123 124 124 124 125 125 125 126 126 126 127 127 127 128 128 128 128 129 129 129 130 130 130 131 131 131 132 132 132 133 133 133 133 134 134 134 135 135 135 136 136 136 137 137 137 138 138 138 138 139 139 139 140 140 140 141 141 141 142 142 142 143 143 143 143 144 144 144 145 145 145 146 146 146 147 147 147 148 148 148 149 149 149 149 150 150 150 151 151 151 152 152 152 153 153 153 154 154 154 155 155 155 155 156 156 156 157 157 157 158 158 158 159 159 159 160 160 160 161 161 161 161 162 162 162 163 163 163 164 164 164 165 165 165 166 166 166 167 167 167 167 168 168 168 169 169 169 170 170 170 171 171 171 172 172 172 173 173 173 174 174 174 174 175 175 175 176 176 176 177 177 177 178 178 178 179 179 179 180 180 180 181 181 181 181 182 182 182 183 183 183 184 184 184 185 185 185 186 186 186 187 187 187 188 188 188 189 189 189 189 190 190 190 191 191 191 192 192 192 193 193 193 194 194 194 195 195 195 196 196 196 197 197 197 197 198 198 198 199 199 199 200 200 200 201 201 201 202 202 202 203 203 203 204 204 204 205 205 205 206 206 206 207 207 207 207 208 208 208 209 209 209 210 210 210 211 211 211 212 212 212 213 213 213 214 214 214 215 215 215 216 216 216 217 217 217 218 218 218 218 219 219 219 220 220 220 221 221 221 222 222 222 223 223 223 224 224 224 225 225 225 226 226 226 227 227 227 228 228 228 229 229 229 230 230 230 231 231 231 232 232 232 232 233 233 233 234 234 234 235 235 235 236 236 236 237 237 237 238 238 238 239 239 239 240 240 240 241 241 241 242 242 242 243 243 243 244 244 244 245 245 245 246 246 246 247 247 247 248 248 248 249 249 249 250 250 250 251 251 251 252 252 252 253 253 253 254 254 254 255 255 255 256 256 256 256 257 257 257 258 258 258 259 259 259 260 260 260 261 261 261 262 262 262 263 263 263 264 264 264 265 265 265 266 266 266 267 267 267 268 268 268 269 269 269 270 270 270 271 271 271 272 272 272 273 273 273 274 274 274 275 275 275 276 276 276 277 277 277 278 278 278 279 279 279 280 280 280 281 281 281 282 282 282 283 283 283 284 284 284 285 285 285 286 286 286 287 287 287 288 288 289 289 289 290 290 290 291 291 291 292 292 292 293 293 293 294 294 294 295 295 295 296 296 296 297 297 297 298 298 298 299 299 299 300 300 300 301 301 301 302 302 302 303 303 303 304 304 304 305 305 305 306 306 306 307 307 307 308 308 308 309 309 309 310 310 310 311 311 312 312 312 313 313 313 314 314 314 315 315 315 316 316 316 317 317 317 318 318 318 319 319 319 320 320 320 321 321 321 322 322 322 323 323 323 324 324 325 325 325 326 326 326 327 327 327 328 328 328 329 329 329 330 330 330 331 331 331 332 332 332 333 333 333 334 334 335 335 335 336 336 336 337 337 337 338 338 338 339 339 339 340 340 340 341 341 341 342 342 342 343 343 344 344 344 345 345 345 346 346 346 347 347 347 348 348 348 349 349 349 350 350 350 351 351 352 352 352 353 353 353 354 354 354 355 355 355 356 356 356 357 357 357 358 358 359 359 359 360 360 360 361 361 361 362 362 362 363 363 363 364 364 364 365 365 366 366 366 367 367 367 368 368 368 369 369 369 370 370 370 371 371 372 372 372 373 373 373 374 374 374 375 375 375 376 376 377 377 377 378 378 378 379 379 379 380 380 380 381 381 381 382 382 383 383 383 384 384 384 385 385 385 386 386 386 387 387 388 388 388 389 389 389 390 390 390 391 391 392 392 392 393 393 393 394 394 394 395 395 395 396 396 397 397 397 398 398 398 399 399 399 400 400 400 401 401 402 402 402 403 403 403 404 404 404 405 405 406 406 406 407 407 407 408 408 408 409 409 410 410 410 411 411 411 412 412 412 413 413 414 414 414 415 415 415 416 416 416 417 417 418 418 418 419 419 419 420 420 421 421 421 422 422 422 423 423 423 424 424 425 425 425 426 426 426 427 427 427 428 428 429 429 429 430 430 430 431 431 432 432 432 433 433 433 434 434 435 435 435 436 436 436 437 437 437 438 438 439 439 439 440 440 440 441 441 442 442 442 443 443 443 444 444 445 445 445 446 446 446 447 447 448 448 448 449 449 449 450 450 451 451 451 452 452 452 453 453 454 454 454 455 455 455 456 456 457 457 457 458 458 458 459 459 460 460 460 461 461 461 462 462 463 463 463 464 464 465 465 465 466 466 466 467 467 468 468 468 469 469 469 470 470 471 471 471 472 472 473 473 473 474 474 474 475 475 476 476 476 477 477 478 478 478 479 479 479 480 480 481 481 481 482 482 483 483 483 484 484 484 485 485 486 486 486 487 487 488 488 488 489 489 489 490 490 491 491 491 492 492 493 493 493 494 494 495 495 495 496 496 497 497 497 498 498 498 499 499 500 500 500 501 501 502 502 502 503 503 504 504 504 505 505 506 506 506 507 507 508 508 508 509 509 509 510 510 511 511 511 512 512 513 513 513 514 514 515 515 515 516 516 517 517 517 518 518 519 519 519 520 520 521 521 521 522 522 523 523 523 524 524 525 525 525 526 526 527 527 527 528 528 529 529 529 530 530 531 531 531 532 532 533 533 534 534 534 535 535 536 536 536 537 537 538 538 538 539 539 540 540 540 541 541 542 542 542 543 543 544 544 545 545 545 546 546 547 547 547 548 548 549 549 549 550 550 551 551 551 552 552 553 553 554 554 554 555 555 556 556 556 557 557 558 558 559 559 559 560 560 561 561 561 562 562 563 563 564 564 564 565 565 566 566 566 567 567 568 568 569 569 569 570 570 571 571 572 572 572 573 573 574 574 574 575 575 576 576 577 577 577 578 578 579 579 580 580 580 581 581 582 582 583 583 583 584 584 585 585 586 586 586 587 587 588 588 589 589 589 590 590 591 591 592 592 592 593 593 594 594 595 595 595 596 596 597 597 598 598 599 599 599 600 600 601 601 602 602 602 603 603 604 604 605 605 605 606 606 607 607 608 608 609 609 609 610 610 611 611 612 612 613 613 613 614 614 615 615 616 616 617 617 617 618 618 619 619 620 620 621 621 621 622 622 623 623 624 624 625 625 625 626 626 627 627 628 628 629 629 630 630 630 631 631 632 632 633 633 634 634 635 635 635 636 636 637 637 638 638 639 639 640 640 640 641 641 642 642 643 643 644 644 645 645 646 646 646 647 647 648 648 649 649 650 650 651 651 652 652 652 653 653 654 654 655 655 656 656 657 657 658 658 659 659 659 660 660 661 661 662 662 663 663 664 664 665 665 666 666 667 667 668 668 668 669 669 670 670 671 671 672 672 673 673 674 674 675 675 676 676 677 677 678 678 679 679 680 680 680 681 681 682 682 683 683 684 684 685 685 686 686 687 687 688 688 689 689 690 690 691 691 692 692 693 693 694 694 695 695 696 696 697 697 698 698 699 699 700 700 701 701 702 702 703 703 704 704 705 705 706 706 707 707 708 708 709 709 710 710 711 711 712 712 713 713 714 714 715 715 716 716 717 717 718 718 719 719 720 720 721 721 722 722 723 723 724 725 725 726 726 727 727 728 728 729 729 730 730 731 731 732 732 733 733 734 734 735 735 736 737 737 738 738 739 739 740 740 741 741 742 742 743 743 744 745 745 746 746 747 747 748 748 749 749 750 750 751 752 752 753 753 754 754 755 755 756 756 757 758 758 759 759 760 760 761 761 762 762 763 764 764 765 765 766 766 767 767 768 769 769 770 770 771 771 772 773 773 774 774 775 775 776 776 777 778 778 779 779 780 780 781 782 782 783 783 784 784 785 786 786 787 787 788 788 789 790 790 791 791 792 793 793 794 794 795 795 796 797 797 798 798 799 800 800 801 801 802 803 803 804 804 805 806 806 807 807 808 809 809 810 810 811 812 812 813 813 814 815 815 816 816 817 818 818 819 820 820 821 821 822 823 823 824 825 825 826 826 827 828 828 829 830 830 831 831 832 833 833 834 835 835 836 836 837 838 838 839 840 840 841 842 842 843 844 844 845 846 846 847 847 848 849 849 850 851 851 852 853 853 854 855 855 856 857 857 858 859 859 860 861 861 862 863 864 864 865 866 866 867 868 868 869 870 870 871 872 872 873 874 875 875 876 877 877 878 879 879 880 881 882 882 883 884 884 885 886 887 887 888 889 889 890 891 892 892 893 894 895 895 896 897 897 898 899 900 900 901 902 903 903 904 905 906 906 907 908 909 910 910 911 912 913 913 914 915 916 916 917 918 919 920 920 921 922 923 924 924 925 926 927 928 928 929 930 931 932 932 933 934 935 936 937 937 938 939 940 941 942 942 943 944 945 946 947 947 948 949 950 951 952 953 953 954 955 956 957 958 959 960 961 961 962 963 964 965 966 967 968 969 970 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1017 1018 1019 1020 1021 1022 1023 1024 1026 1027 1028 1029 1030 1031 1032 1034 1035 1036 1037 1038 1040 1041 1042 1043 1045 1046 1047 1048 1050 1051 1052 1054 1055 1056 1057 1059 1060 1062 1063 1064 1066 1067 1068 1070 1071 1073 1074 1076 1077 1079 1080 1082 1083 1085 1086 1088 1090 1091 1093 1094 1096 1098 1100 1101 1103 1105 1107 1108 1110 1112 1114 1116 1118 1120 1122 1124 1126 1128 1131 1133 1135 1138 1140 1142 1145 1148 1150 1153 1156 1159 1163 1166 1170 1174 1178 1184\n",
      "  cuda GPU #0 \"Tesla V100-SXM2-32GB\" found (warp size = 32, compute capabilities = 7.0, CU=80, max_threads=163840, 31.7 GB, L2 cache=6MB).\n",
      "!!! Use theta-contiguous FFT on GPU !!!\n",
      "=> Using VkFFT v1.2.33\n",
      "        + GPU #0 successfully initialized.\n",
      "        scalar SH - poloidal   rms error = 0.000336  max error = 0.0757 for l=2558,lm=2558\n",
      "        vector SH - spheroidal rms error = nan  max error = 4.66e-08 for l=2288,lm=401407\n",
      "                  - toroidal   rms error = nan  max error = 4.21e-08 for l=2048,lm=367490\n",
      "        + SHT accuracy = 0.0757\n",
      "\u001b[93m Accuracy test failed. Please file a bug report at https://bitbucket.org/nschaeff/shtns/issues \u001b[0m\n",
      "\u001b[93m You may need to upgrade the 'binutils' package, see https://bitbucket.org/nschaeff/shtns/issues/37/ \u001b[0m\n",
      " nthreads = 10\n",
      "        => SHTns is ready.\n",
      "[SHTns 3.6.6] built May 27 2024, 07:48:01, id: v3.6.6-11-gf0a006a,avx512,ishioka,openmp,cuda\n",
      "        Lmax=2559, Mmax*Mres=2559, Mres=1, Nlm=3278080  [10 threads, orthonormalized]\n",
      "        => using FFTW : Mmax=2559, Nphi=5120, Nlat=2560, Nbatch=1  (theta-contiguous layout: phi_inc=2560, theta_inc=1)\n",
      "          fftw cost ifftc=1.09287e+08,  fftc=1.09287e+08\n",
      "          Memory used for Ylm and Zlm matrices = 96037.500 Mb x2\n",
      "        => using Gauss nodes\n",
      "          Sum of weights = 2 + -4.44089e-16 (should be 2)\n",
      "          Applying quadrature rule to 3/2.x^2 = 1 + -2.22045e-16 (should be 1)\n",
      "          Applying quadrature rule to 3/4.sin2(theta) = 1 + -5.55112e-16 (should be 1)\n",
      " NLAT=2560, NLAT_2=1280\n",
      "          max zero at Gauss nodes for Pl[l=NLAT] : 6.61376e-11\n",
      "        + polar optimization threshold = 1.0e-10\n",
      "          tm[im]= 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 5 5 6 6 6 6 7 7 7 7 8 8 8 8 9 9 9 9 10 10 10 10 11 11 11 11 12 12 12 12 13 13 13 13 14 14 14 14 15 15 15 16 16 16 16 17 17 17 17 18 18 18 18 19 19 19 20 20 20 20 21 21 21 21 22 22 22 23 23 23 23 24 24 24 24 25 25 25 26 26 26 26 27 27 27 28 28 28 28 29 29 29 30 30 30 30 31 31 31 32 32 32 32 33 33 33 34 34 34 34 35 35 35 36 36 36 36 37 37 37 38 38 38 38 39 39 39 40 40 40 40 41 41 41 42 42 42 42 43 43 43 44 44 44 44 45 45 45 46 46 46 46 47 47 47 48 48 48 49 49 49 49 50 50 50 51 51 51 51 52 52 52 53 53 53 53 54 54 54 55 55 55 56 56 56 56 57 57 57 58 58 58 59 59 59 59 60 60 60 61 61 61 61 62 62 62 63 63 63 64 64 64 64 65 65 65 66 66 66 67 67 67 67 68 68 68 69 69 69 69 70 70 70 71 71 71 72 72 72 72 73 73 73 74 74 74 75 75 75 75 76 76 76 77 77 77 78 78 78 78 79 79 79 80 80 80 81 81 81 81 82 82 82 83 83 83 84 84 84 84 85 85 85 86 86 86 87 87 87 87 88 88 88 89 89 89 90 90 90 91 91 91 91 92 92 92 93 93 93 94 94 94 94 95 95 95 96 96 96 97 97 97 97 98 98 98 99 99 99 100 100 100 101 101 101 101 102 102 102 103 103 103 104 104 104 104 105 105 105 106 106 106 107 107 107 108 108 108 108 109 109 109 110 110 110 111 111 111 111 112 112 112 113 113 113 114 114 114 115 115 115 115 116 116 116 117 117 117 118 118 118 119 119 119 119 120 120 120 121 121 121 122 122 122 123 123 123 123 124 124 124 125 125 125 126 126 126 127 127 127 127 128 128 128 129 129 129 130 130 130 131 131 131 131 132 132 132 133 133 133 134 134 134 135 135 135 135 136 136 136 137 137 137 138 138 138 139 139 139 140 140 140 140 141 141 141 142 142 142 143 143 143 144 144 144 144 145 145 145 146 146 146 147 147 147 148 148 148 149 149 149 149 150 150 150 151 151 151 152 152 152 153 153 153 154 154 154 154 155 155 155 156 156 156 157 157 157 158 158 158 159 159 159 159 160 160 160 161 161 161 162 162 162 163 163 163 164 164 164 164 165 165 165 166 166 166 167 167 167 168 168 168 169 169 169 169 170 170 170 171 171 171 172 172 172 173 173 173 174 174 174 175 175 175 175 176 176 176 177 177 177 178 178 178 179 179 179 180 180 180 180 181 181 181 182 182 182 183 183 183 184 184 184 185 185 185 186 186 186 186 187 187 187 188 188 188 189 189 189 190 190 190 191 191 191 192 192 192 193 193 193 193 194 194 194 195 195 195 196 196 196 197 197 197 198 198 198 199 199 199 200 200 200 200 201 201 201 202 202 202 203 203 203 204 204 204 205 205 205 206 206 206 207 207 207 207 208 208 208 209 209 209 210 210 210 211 211 211 212 212 212 213 213 213 214 214 214 215 215 215 215 216 216 216 217 217 217 218 218 218 219 219 219 220 220 220 221 221 221 222 222 222 223 223 223 224 224 224 225 225 225 225 226 226 226 227 227 227 228 228 228 229 229 229 230 230 230 231 231 231 232 232 232 233 233 233 234 234 234 235 235 235 235 236 236 236 237 237 237 238 238 238 239 239 239 240 240 240 241 241 241 242 242 242 243 243 243 244 244 244 245 245 245 246 246 246 247 247 247 248 248 248 248 249 249 249 250 250 250 251 251 251 252 252 252 253 253 253 254 254 254 255 255 255 256 256 256 257 257 257 258 258 258 259 259 259 260 260 260 261 261 261 262 262 262 263 263 263 264 264 264 265 265 265 266 266 266 267 267 267 267 268 268 268 269 269 269 270 270 270 271 271 271 272 272 272 273 273 273 274 274 274 275 275 275 276 276 276 277 277 277 278 278 278 279 279 279 280 280 280 281 281 281 282 282 282 283 283 283 284 284 284 285 285 285 286 286 286 287 287 287 288 288 288 289 289 289 290 290 290 291 291 291 292 292 292 293 293 293 294 294 294 295 295 295 296 296 296 297 297 297 298 298 298 299 299 299 300 300 300 301 301 301 302 302 302 303 303 303 304 304 304 305 305 305 306 306 306 307 307 307 308 308 308 309 309 309 310 310 310 311 311 311 312 312 313 313 313 314 314 314 315 315 315 316 316 316 317 317 317 318 318 318 319 319 319 320 320 320 321 321 321 322 322 322 323 323 323 324 324 324 325 325 325 326 326 326 327 327 327 328 328 328 329 329 329 330 330 331 331 331 332 332 332 333 333 333 334 334 334 335 335 335 336 336 336 337 337 337 338 338 338 339 339 339 340 340 340 341 341 341 342 342 343 343 343 344 344 344 345 345 345 346 346 346 347 347 347 348 348 348 349 349 349 350 350 350 351 351 351 352 352 353 353 353 354 354 354 355 355 355 356 356 356 357 357 357 358 358 358 359 359 359 360 360 361 361 361 362 362 362 363 363 363 364 364 364 365 365 365 366 366 366 367 367 367 368 368 369 369 369 370 370 370 371 371 371 372 372 372 373 373 373 374 374 374 375 375 376 376 376 377 377 377 378 378 378 379 379 379 380 380 380 381 381 382 382 382 383 383 383 384 384 384 385 385 385 386 386 387 387 387 388 388 388 389 389 389 390 390 390 391 391 391 392 392 393 393 393 394 394 394 395 395 395 396 396 396 397 397 398 398 398 399 399 399 400 400 400 401 401 401 402 402 403 403 403 404 404 404 405 405 405 406 406 407 407 407 408 408 408 409 409 409 410 410 410 411 411 412 412 412 413 413 413 414 414 414 415 415 416 416 416 417 417 417 418 418 418 419 419 420 420 420 421 421 421 422 422 422 423 423 424 424 424 425 425 425 426 426 426 427 427 428 428 428 429 429 429 430 430 430 431 431 432 432 432 433 433 433 434 434 435 435 435 436 436 436 437 437 437 438 438 439 439 439 440 440 440 441 441 442 442 442 443 443 443 444 444 445 445 445 446 446 446 447 447 447 448 448 449 449 449 450 450 450 451 451 452 452 452 453 453 453 454 454 455 455 455 456 456 456 457 457 458 458 458 459 459 459 460 460 461 461 461 462 462 462 463 463 464 464 464 465 465 465 466 466 467 467 467 468 468 468 469 469 470 470 470 471 471 472 472 472 473 473 473 474 474 475 475 475 476 476 476 477 477 478 478 478 479 479 480 480 480 481 481 481 482 482 483 483 483 484 484 485 485 485 486 486 486 487 487 488 488 488 489 489 490 490 490 491 491 491 492 492 493 493 493 494 494 495 495 495 496 496 497 497 497 498 498 498 499 499 500 500 500 501 501 502 502 502 503 503 504 504 504 505 505 505 506 506 507 507 507 508 508 509 509 509 510 510 511 511 511 512 512 513 513 513 514 514 515 515 515 516 516 517 517 517 518 518 519 519 519 520 520 520 521 521 522 522 522 523 523 524 524 524 525 525 526 526 526 527 527 528 528 528 529 529 530 530 530 531 531 532 532 533 533 533 534 534 535 535 535 536 536 537 537 537 538 538 539 539 539 540 540 541 541 541 542 542 543 543 543 544 544 545 545 545 546 546 547 547 548 548 548 549 549 550 550 550 551 551 552 552 552 553 553 554 554 555 555 555 556 556 557 557 557 558 558 559 559 559 560 560 561 561 562 562 562 563 563 564 564 564 565 565 566 566 567 567 567 568 568 569 569 569 570 570 571 571 572 572 572 573 573 574 574 575 575 575 576 576 577 577 577 578 578 579 579 580 580 580 581 581 582 582 583 583 583 584 584 585 585 586 586 586 587 587 588 588 589 589 589 590 590 591 591 592 592 592 593 593 594 594 595 595 595 596 596 597 597 598 598 598 599 599 600 600 601 601 602 602 602 603 603 604 604 605 605 605 606 606 607 607 608 608 609 609 609 610 610 611 611 612 612 612 613 613 614 614 615 615 616 616 616 617 617 618 618 619 619 620 620 620 621 621 622 622 623 623 624 624 625 625 625 626 626 627 627 628 628 629 629 629 630 630 631 631 632 632 633 633 634 634 634 635 635 636 636 637 637 638 638 639 639 639 640 640 641 641 642 642 643 643 644 644 645 645 645 646 646 647 647 648 648 649 649 650 650 651 651 651 652 652 653 653 654 654 655 655 656 656 657 657 658 658 658 659 659 660 660 661 661 662 662 663 663 664 664 665 665 666 666 666 667 667 668 668 669 669 670 670 671 671 672 672 673 673 674 674 675 675 676 676 677 677 677 678 678 679 679 680 680 681 681 682 682 683 683 684 684 685 685 686 686 687 687 688 688 689 689 690 690 691 691 692 692 693 693 694 694 695 695 696 696 697 697 698 698 699 699 700 700 701 701 702 702 703 703 704 704 705 705 706 706 707 707 708 708 709 709 710 710 711 711 712 712 713 713 714 714 715 715 716 716 717 717 718 718 719 719 720 720 721 721 722 722 723 723 724 725 725 726 726 727 727 728 728 729 729 730 730 731 731 732 732 733 733 734 734 735 736 736 737 737 738 738 739 739 740 740 741 741 742 742 743 744 744 745 745 746 746 747 747 748 748 749 749 750 751 751 752 752 753 753 754 754 755 755 756 757 757 758 758 759 759 760 760 761 762 762 763 763 764 764 765 765 766 767 767 768 768 769 769 770 771 771 772 772 773 773 774 774 775 776 776 777 777 778 778 779 780 780 781 781 782 782 783 784 784 785 785 786 787 787 788 788 789 789 790 791 791 792 792 793 794 794 795 795 796 797 797 798 798 799 800 800 801 801 802 803 803 804 804 805 806 806 807 807 808 809 809 810 810 811 812 812 813 813 814 815 815 816 817 817 818 818 819 820 820 821 822 822 823 823 824 825 825 826 827 827 828 828 829 830 830 831 832 832 833 834 834 835 835 836 837 837 838 839 839 840 841 841 842 843 843 844 845 845 846 847 847 848 849 849 850 851 851 852 853 853 854 855 855 856 857 857 858 859 859 860 861 861 862 863 863 864 865 866 866 867 868 868 869 870 870 871 872 873 873 874 875 875 876 877 878 878 879 880 880 881 882 883 883 884 885 885 886 887 888 888 889 890 891 891 892 893 894 894 895 896 897 897 898 899 900 900 901 902 903 903 904 905 906 906 907 908 909 910 910 911 912 913 914 914 915 916 917 917 918 919 920 921 921 922 923 924 925 926 926 927 928 929 930 931 931 932 933 934 935 936 936 937 938 939 940 941 942 942 943 944 945 946 947 948 948 949 950 951 952 953 954 955 956 956 957 958 959 960 961 962 963 964 965 966 967 968 969 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1012 1013 1014 1015 1016 1017 1018 1020 1021 1022 1023 1024 1025 1027 1028 1029 1030 1032 1033 1034 1035 1037 1038 1039 1040 1042 1043 1044 1046 1047 1048 1050 1051 1052 1054 1055 1057 1058 1059 1061 1062 1064 1065 1067 1068 1070 1071 1073 1074 1076 1078 1079 1081 1082 1084 1086 1088 1089 1091 1093 1095 1096 1098 1100 1102 1104 1106 1108 1110 1112 1114 1117 1119 1121 1123 1126 1128 1131 1133 1136 1139 1142 1145 1148 1152 1155 1159 1164 1169\n",
      "  cuda GPU #0 \"Tesla V100-SXM2-32GB\" found (warp size = 32, compute capabilities = 7.0, CU=80, max_threads=163840, 31.7 GB, L2 cache=6MB).\n",
      "!!! Use theta-contiguous FFT on GPU !!!\n",
      "=> Using VkFFT v1.2.33\n",
      "        + GPU #0 successfully initialized.\n",
      "        scalar SH - poloidal   rms error = 1.54e-13  max error = 3.99e-12 for l=1120,lm=1120\n",
      "        vector SH - spheroidal rms error = 1.54e-13  max error = 4.21e-12 for l=2070,lm=4629\n",
      "                  - toroidal   rms error = 1.52e-13  max error = 4.25e-12 for l=2252,lm=4811\n",
      "        + SHT accuracy = 4.25e-12\n",
      " nthreads = 10\n",
      "        => SHTns is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2891921/1287995885.py:81: RuntimeWarning: divide by zero encountered in divide\n",
      "  dlm_scaled = hp.almxfl(toydlm, np.nan_to_num(np.sqrt(1/(ll*(ll+1)))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in timing execution mode\n",
      "- - - - - - - - - - - - - - - Timing spin-1 synth: 0.260 seconds- - - - - - - - - - - - - - - \n",
      "\n",
      "- - - - - - - - - - - - - - - Timing pointing: 0.030 seconds- - - - - - - - - - - - - - - \n",
      "\n",
      "- - - - - - - - - - - - - - - Timing dlm2pointing: 0.295 seconds- - - - - - - - - - - - - - - \n",
      "\n",
      "- - - - - - - - - - - - - - - Timing dlm2pointing: 0.590 seconds- - - - - - - - - - - - - - - \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m dlm_scaled \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39matleast_2d(dlm_scaled), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mcomplex128) \u001b[38;5;66;03m#if kwargs['epsilon']<=1e-6 else cp.array(np.atleast_2d(dlm_scaled).astype(np.complex64))\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# defres[backend][solver] = t.gclm2lenmap(cp.array(toyunllm.copy()), dlm_scaled=dlm_scaled, lmax=lmax, mmax=lmax, lenmap=lenmap, ptg=None, execmode='timing', runid=int(sys.argv[4]))\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m defres[backend][solver] \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgclm2lenmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoyunllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlm_scaled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdlm_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlenmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlenmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtiming\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/cunuSHT/cunusht/helper_GPU.py:34\u001b[0m, in \u001b[0;36mtiming_decorator_close.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39madd(tkey)\n\u001b[1;32m     33\u001b[0m cp\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mdeviceSynchronize()\n\u001b[0;32m---> 34\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m cp\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mdeviceSynchronize()\n\u001b[1;32m     36\u001b[0m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39madd_elapsed(tkey)\n",
      "File \u001b[0;32m~/git/cunuSHT/cunusht/deflection/GPU_nufft_transformer.py:549\u001b[0m, in \u001b[0;36mGPU_cufinufft_transformer.gclm2lenmap\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    546\u001b[0m y, x \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray([pointing_theta, pointing_phi])\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m0\u001b[39m], cp\u001b[38;5;241m.\u001b[39marray([pointing_theta, pointing_phi])\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnuFFTplan\u001b[38;5;241m.\u001b[39msetpts(x, y, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 549\u001b[0m lenmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesis_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgclm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpointing_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointing_phi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlenmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtiming\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    552\u001b[0m     t0, ti \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/git/cunuSHT/cunusht/helper_GPU.py:16\u001b[0m, in \u001b[0;36mtiming_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m t0, ti \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     15\u001b[0m cp\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mdeviceSynchronize()\n\u001b[0;32m---> 16\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m cp\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mdeviceSynchronize()\n\u001b[1;32m     18\u001b[0m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39madd(tkey)\n",
      "File \u001b[0;32m~/git/cunuSHT/cunusht/deflection/GPU_nufft_transformer.py:419\u001b[0m, in \u001b[0;36mGPU_cufinufft_transformer.synthesis_general\u001b[0;34m(self, lmax, mmax, alm, loc, pointmap, epsilon, verbose)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# pointmap = pointmap.astype(cp.float32 if epsilon>1e-6 else cp.float64) \u001b[39;00m\n\u001b[1;32m    417\u001b[0m pointing_theta, pointing_phi \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m0\u001b[39m], loc\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# transposing so that we follow DUCC convention\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_synthesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43malm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCARmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m alm\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m#TODO 2d-doubling would be nice-to-have at some point\u001b[39;00m\n",
      "File \u001b[0;32m~/git/cunuSHT/cunusht/helper_GPU.py:79\u001b[0m, in \u001b[0;36mdebug_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mexecmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     81\u001b[0m     buff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/cunuSHT/cunusht/helper_GPU.py:16\u001b[0m, in \u001b[0;36mtiming_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m t0, ti \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     15\u001b[0m cp\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mdeviceSynchronize()\n\u001b[0;32m---> 16\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m cp\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mdeviceSynchronize()\n\u001b[1;32m     18\u001b[0m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m.\u001b[39madd(tkey)\n",
      "File \u001b[0;32m~/git/cunuSHT/cunusht/deflection/GPU_nufft_transformer.py:256\u001b[0m, in \u001b[0;36mGPU_cufinufft_transformer._synthesis\u001b[0;34m(self, alm, out, lmax, mmax)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;129m@debug_decorator\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;129m@timing_decorator\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# @shape_decorator\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_synthesis\u001b[39m(\u001b[38;5;28mself\u001b[39m, alm, out, lmax, mmax):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# This is CAR grid, as init sets up SHT transformer with CAR geometry\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m alm\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mcomplex128, cp\u001b[38;5;241m.\u001b[39mcomplex128], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malm should be double precision for accurate SHT, but is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(alm\u001b[38;5;241m.\u001b[39mdtype) \n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesis_cupy\u001b[49m\u001b[43m(\u001b[49m\u001b[43malm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmmax\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/cunuSHT/cunusht/sht/GPU_sht_transformer.py:60\u001b[0m, in \u001b[0;36mGPU_SHTns_transformer.synthesis_cupy\u001b[0;34m(self, gclm, out, lmax, mmax, mode, nthreads)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynthesis_cupy\u001b[39m(\u001b[38;5;28mself\u001b[39m, gclm, out, lmax, mmax, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#TODO all other than gclm not supported. Want same interface for each backend, \u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# could check grid for each synth and ana call and update if needed\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124;03m\"\"\"Wrapper to SHTns forward SHT\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m        Return a map or a pair of map for spin non-zero, with the same type as gclm\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgclm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/venv/pysht/lib/python3.10/site-packages/shtns.py:300\u001b[0m, in \u001b[0;36msht.synth\u001b[0;34m(self, *arg)\u001b[0m\n\u001b[1;32m    298\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(arg)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,n):\n\u001b[0;32m--> 300\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlm: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectral array has wrong size.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m q[i]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnum \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplex128\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnum: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectral array should be dtype=complex.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    302\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m q[i]\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m: q[i] \u001b[38;5;241m=\u001b[39m q[i]\u001b[38;5;241m.\u001b[39mcopy()\t\t\u001b[38;5;66;03m# contiguous array required.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Benchmark gclm2lenmap by scanning across different solvers, backends, and modes, and for different lmax values.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "import healpy as hp\n",
    "import cunusht\n",
    "import sys\n",
    "from time import process_time\n",
    "\n",
    "runinfos = [\n",
    "    # (\"CPU\", \"lenspyx\"),\n",
    "    # (\"CPU\", \"duccnufft\"),\n",
    "    (\"GPU\", \"cufinufft\")\n",
    "    ]\n",
    "epsilons = [1e-08]\n",
    "# lmaxs = [256*n-1 for n in np.arange(int(sys.argv[1]), 24)]\n",
    "lmaxs = [256*int(10)-1]\n",
    "runinfos = [(\"GPU\", \"cufinufft\")] if 'GPU' == 'GPU' else [(\"CPU\", \"lenspyx\")]\n",
    "phi_lmaxs = [lmax for lmax in lmaxs]\n",
    "defres = {}\n",
    "Tsky = None\n",
    "Tsky2 = None\n",
    "nthreads = 10\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    for runinfo in runinfos:\n",
    "        for lmax, phi_lmax in zip(lmaxs, phi_lmaxs):\n",
    "            geominfo = ('gl',{'lmax':lmax})\n",
    "            lenjob_geominfo = ('gl',{'lmax':phi_lmax})\n",
    "            lldlm = np.arange(0,phi_lmax+1)\n",
    "            if False:\n",
    "                from delensalot.sims.sims_lib import Xunl, Xsky\n",
    "                synunl = Xunl(lmax=lmax, geominfo=geominfo, phi_lmax=phi_lmax)\n",
    "                philm = synunl.get_sim_phi(0, space='alm')\n",
    "                toydlm = hp.almxfl(philm, np.sqrt(lldlm*(lldlm+1)))\n",
    "                toyunllm = synunl.get_sim_unl(0, spin=0, space='alm', field='temperature')\n",
    "            else:\n",
    "                nalm_unl = hp.Alm.getsize(lmax, mmax=lmax)\n",
    "                toyunllm = np.array([np.random.rand(nalm_unl)*1e-6 + 1j*np.random.rand(nalm_unl)*1e-6])\n",
    "                toydlm = np.random.rand(nalm_unl)*1e-6 + 1j*np.random.rand(nalm_unl)*1e-6\n",
    "\n",
    "            backend = runinfo[0]\n",
    "            defres.update({backend: {}}) if backend not in defres.keys() else None\n",
    "            solver = runinfo[1]\n",
    "            defres[backend].update({solver : None}) if solver not in defres[backend].keys() else None\n",
    "            \n",
    "            t = cunusht.get_transformer(backend, solver)\n",
    "            if backend == 'CPU':\n",
    "                if solver == 'lenspyx':\n",
    "                    kwargs = {\n",
    "                        'geominfo_deflection': lenjob_geominfo,\n",
    "                        'dglm': toydlm,\n",
    "                        'mmax_dlm': lmax,\n",
    "                        'nthreads': nthreads,\n",
    "                        'verbose': 1,\n",
    "                        'epsilon': epsilon,\n",
    "                        'single_prec': False,\n",
    "                    }\n",
    "                    t = t(**kwargs)\n",
    "                    defres[backend][solver] = t.gclm2lenmap(\n",
    "                            toyunllm.copy(), dlm=toydlm, lmax=lmax, mmax=lmax, spin=0, nthreads=10, execmode='timing', ptg=None)\n",
    "                else:\n",
    "                    kwargs = {\n",
    "                        'geominfo_deflection': lenjob_geominfo,\n",
    "                        'nuFFTtype': 2,\n",
    "                    }\n",
    "                    t = t(**kwargs)\n",
    "                    defres[backend][solver] = t.gclm2lenmap(\n",
    "                            gclm=toyunllm.copy(), dlm=toydlm, lmax=lmax, mmax=lmax, spin=0, nthreads=nthreads, epsilon=epsilon, execmode='timing', ptg=None)\n",
    "            elif backend == 'GPU':\n",
    "                import cupy as cp\n",
    "                kwargs = {\n",
    "                    'geominfo_deflection': lenjob_geominfo,\n",
    "                    'epsilon': epsilon,\n",
    "                    'nuFFTtype': 2,\n",
    "                }\n",
    "                t = t(**kwargs)\n",
    "                lenmap = cp.empty(t.deflectionlib.constructor.spat_shape, dtype=cp.complex128).flatten()\n",
    "                ll = np.arange(0,lmax+1,1)\n",
    "                dlm_scaled = hp.almxfl(toydlm, np.nan_to_num(np.sqrt(1/(ll*(ll+1)))))\n",
    "                dlm_scaled = cp.array(np.atleast_2d(dlm_scaled), dtype=np.complex128) #if kwargs['epsilon']<=1e-6 else cp.array(np.atleast_2d(dlm_scaled).astype(np.complex64))\n",
    "                # defres[backend][solver] = t.gclm2lenmap(cp.array(toyunllm.copy()), dlm_scaled=dlm_scaled, lmax=lmax, mmax=lmax, lenmap=lenmap, ptg=None, execmode='timing', runid=int(sys.argv[4]))\n",
    "                defres[backend][solver] = t.gclm2lenmap(toyunllm.copy(), dlm_scaled=dlm_scaled, lmax=lmax, mmax=lmax, lenmap=lenmap, ptg=None, execmode='timing', runid=int(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  C_CONTIGUOUS : True\n",
       "  F_CONTIGUOUS : True\n",
       "  OWNDATA : True\n",
       "  WRITEABLE : True\n",
       "  ALIGNED : True\n",
       "  WRITEBACKIFCOPY : False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "a = cp.empty(10)\n",
    "\n",
    "a.flags\n",
    "\n",
    "b = np.empty(10)\n",
    "b.flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
