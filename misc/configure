module load python cuda; source $pyshtenv; cd cunusht/c/

nvcc  -o assocLeg.so -shared -Xcompiler -fPIC assocLeg.cu
nvcc  -o cunusht/c/pointing.so -shared -Xcompiler -fPIC cunusht/c/pointing.cpp cunusht/c/pointing.cu

srun python3 -m notebook --port=1234 --ip=$hostname

# bind the c code to python
cmake -S . -B build
cmake --build build

to install SHTns on the rusty cluster, make sure you the following to variables, otherwise SHTns will 'install' but not have updated GPU routines (not sure why)
export CUDA_PATH=/mnt/sw/nix/store/zi2wc26znf75csf5hhz77p0d2bbz53ih-cuda-11.8.0
export LD_LIBRARY_PATH=/mnt/sw/nix/store/zi2wc26znf75csf5hhz77p0d2bbz53ih-cuda-11.8.0/lib64


## Rusty @ Simons

```
salloc -p gpu --gpus=1 -C v100 -c 1
ssh sbelkner@<workergpuX>
remember workerid!
module load python gcc cuda
./start_jupyter (don't miss half the token..)
connect via VS Code kernel
```


## Ygdrassil
Quickhelp to activate GPU on jupyter notebook in VS code


Activate conda environment, and load modules before starting kernel,

```
conda activate shtgpu
source load_modules
```

Allocate GPU,
```
salloc --gpus 1 --partition=shared-gpu --time=320:00
```

Run script for starting server,

```
./start_jupyter.sh
```

In VS Code, choose exsiting server with URL.
Finally, in notebook, choose shtgpu kernel.


To install shtns, activate gpu node, load compatible modules (CUDAcore needs gcc <=10, i.e: module load GCC/9.3.0)

There appears to be two shtgpu conda environs. One is on login node, the other is on gpu node. Install stuff into the gpu node version, as only there shtns and cufinufft installations succeed.